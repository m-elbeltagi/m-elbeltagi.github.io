## Education
- Ph.D., Particle Physics|Carleton University, Canada
- BSc Honours, Theoretical Physics|Carleton University, Canada
  
---

## Professional Experience

**Research Assistant/Data Analyst @ Carleton University - Physics Department (_May 2018 - Present_)**
- Spearheaded the collection, storage and analysis of laboratory sensor data using **Python** and LabVIEW, enhancing the operational efficiency of particle physics experiment through uninterrupted monitoring and maintenance of a complex detector system.
- Developed a **Python** package for signal processing and time-series analysis (with **visualization** and **reports**), enabling pattern recognition and predictive maintenance that significantly reduced the risk of dielectric breakdowns in HV equipment.
- Applied innovative Monte Carlo methods in **machine learning** to improve probability density estimation for detector events, which sharpened the precision of measurement uncertainty. 


**NLP Engineer Intern @ Advanced Symbolics (Summer 2023)**
- Reasearched latest papers to engineer and integrate custom metrics to meticulously evaluate **NLP** tasks, including clustering and summarization, ensuring high-quality model output.
- Actively engaged in **MLOps** practices, orchestrating the seamless operation of **NLP** production pipelines on **AirFlow** (using **Git**), significantly minimizing downtime and bolstering continuous delivery of **machine learning** solutions.

**Physics Course Instructor @ Carleton University - Math Department (_Sep 2021 - Present_)**
- Led full-year physics courses on mechanics and electrostatics for high school students, developing and delivering lecture materials, coordinating with TAs for optimal student support.

---
## Applied Projects
###Twitter Troll Detection

[Project Poster](/resources/TwitterTroll_poster.png).

[**_The attached poster won 3rd place at the Carleton Data Day 9.0 poster competition._**](https://science.carleton.ca/dataday9/)

We apply a transfer learning approach to tweet classification, 3 models were used to compare their performance. (1) A distilled version of a pretrained LLM with fixed weights for encoding the text, and a binary classification head trained on top of it. (2) A pretrained smaller version of a LLM, now its weights adjustable by the training process to be finetuned, along with a binary classification head. (3) and finally a new (at the time this project was underway) few-shot learning (without prompts) technique called SetFit (https://huggingface.co/blog/setfit) that dramatically decreases the number of examples needed for training by using contrastive learning. The models were trained using 2 political tweets datasets, one confirmed to be russian-troll tweets, and one containing election-related political tweets, while excluding foreign/ and non-english language tweets.

Visit my GitHub Profile to view the project code.
<img src="/resources/TwitterTroll_cover.png?raw=true" width="512" height="512"/>

---
[Project 2 Title](/pdf/sample_presentation.pdf)
<img src="images/dummy_thumbnail.jpg?raw=true"/>

---
[Project 3 Title](http://example.com/)
<img src="images/dummy_thumbnail.jpg?raw=true"/>

---

### Category Name 2

- [Project 1 Title](http://example.com/)
- [Project 2 Title](http://example.com/)
- [Project 3 Title](http://example.com/)
- [Project 4 Title](http://example.com/)
- [Project 5 Title](http://example.com/)


